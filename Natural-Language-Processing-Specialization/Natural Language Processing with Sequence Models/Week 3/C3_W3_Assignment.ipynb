{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7oVbe_pLr3C"
   },
   "source": [
    "# Assignment 3 - Named Entity Recognition (NER)\n",
    "\n",
    "Welcome to the third programming assignment of Course 3. In this assignment, you will learn to build more complicated models with Trax. By completing this assignment, you will be able to: \n",
    "\n",
    "- Design the architecture of a neural network, train it, and test it. \n",
    "- Process features and represents them\n",
    "- Understand word padding\n",
    "- Implement LSTMs\n",
    "- Test with your own sentence\n",
    "\n",
    "## Outline\n",
    "- [Introduction](#0)\n",
    "- [Part 1:  Exploring the data](#1)\n",
    "    - [1.1  Importing the Data](#1.1)\n",
    "    - [1.2  Data generator](#1.2)\n",
    "\t\t- [Exercise 01](#ex01)\n",
    "- [Part 2:  Building the model](#2)\n",
    "\t- [Exercise 02](#ex02)\n",
    "- [Part 3:  Train the Model ](#3)\n",
    "\t- [Exercise 03](#ex03)\n",
    "- [Part 4:  Compute Accuracy](#4)\n",
    "\t- [Exercise 04](#ex04)\n",
    "- [Part 5:  Testing with your own sentence](#5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ftT-5-yynCtl"
   },
   "source": [
    "<a name=\"0\"></a>\n",
    "# Introduction\n",
    "\n",
    "We first start by defining named entity recognition (NER). NER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. \n",
    "\n",
    "For example:\n",
    "\n",
    "<img src = 'ner.png' width=\"width\" height=\"height\" style=\"width:600px;height:150px;\"/>\n",
    "\n",
    "Is labeled as follows: \n",
    "\n",
    "- French: geopolitical entity\n",
    "- Morocco: geographic entity \n",
    "- Christmas: time indicator\n",
    "\n",
    "Everything else that is labeled with an `O` is not considered to be a named entity. In this assignment, you will train a named entity recognition system that could be trained in a few seconds (on a GPU) and will get around 75% accuracy. Then, you will load in the exact version of your model, which was trained for a longer period of time. You could then evaluate the trained version of your model to get 96% accuracy! Finally, you will be able to test your named entity recognition system with your own sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "JEY_jlQQR9SP",
    "outputId": "825f37fd-cf03-483a-a6b1-3d70da6f70f1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip -q install trax==1.3.1\n",
    "\n",
    "import trax \n",
    "from trax import layers as tl\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from utils import get_params, get_vocab\n",
    "import random as rnd\n",
    "\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "# trax.supervised.trainer_lib.init_random_number_generators(33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_PpjG5MuLr3F"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "# Part 1:  Exploring the data\n",
    "\n",
    "We will be using a dataset from Kaggle, which we will preprocess for you. The original data consists of four columns, the sentence number, the word, the part of speech of the word, and the tags.  A few tags you might expect to see are: \n",
    "\n",
    "* geo: geographical entity\n",
    "* org: organization\n",
    "* per: person \n",
    "* gpe: geopolitical entity\n",
    "* tim: time indicator\n",
    "* art: artifact\n",
    "* eve: event\n",
    "* nat: natural phenomenon\n",
    "* O: filler word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "-Jur1JnXnCtr",
    "outputId": "88584ab6-0a15-489b-db5b-eb474e129c4f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "SENTENCE LABEL: O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n",
      "ORIGINAL DATA:\n",
      "     Sentence #           Word  POS Tag\n",
      "0  Sentence: 1      Thousands  NNS   O\n",
      "1          NaN             of   IN   O\n",
      "2          NaN  demonstrators  NNS   O\n",
      "3          NaN           have  VBP   O\n",
      "4          NaN        marched  VBN   O\n"
     ]
    }
   ],
   "source": [
    "# display original kaggle data\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding = \"ISO-8859-1\") \n",
    "train_sents = open('data/small/train/sentences.txt', 'r').readline()\n",
    "train_labels = open('data/small/train/labels.txt', 'r').readline()\n",
    "print('SENTENCE:', train_sents)\n",
    "print('SENTENCE LABEL:', train_labels)\n",
    "print('ORIGINAL DATA:\\n', data.head(5))\n",
    "# del(data, train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xoH6yBWVfzTb"
   },
   "source": [
    "<a name=\"1.1\"></a>\n",
    "## 1.1  Importing the Data\n",
    "\n",
    "In this part, we will import the preprocessed data and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UauHjIKHWC0N",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab, tag_map = get_vocab('data/large/words.txt', 'data/large/tags.txt')\n",
    "t_sentences, t_labels, t_size = get_params(vocab, tag_map, 'data/large/train/sentences.txt', 'data/large/train/labels.txt')\n",
    "v_sentences, v_labels, v_size = get_params(vocab, tag_map, 'data/large/val/sentences.txt', 'data/large/val/labels.txt')\n",
    "test_sentences, test_labels, test_size = get_params(vocab, tag_map, 'data/large/test/sentences.txt', 'data/large/test/labels.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcQi6EmWnCty"
   },
   "source": [
    "`vocab` is a dictionary that translates a word string to a unique number. Given a sentence, you can represent it as an array of numbers translating with this dictionary. The dictionary contains a `<PAD>` token. \n",
    "\n",
    "When training an LSTM using batches, all your input sentences must be the same size. To accomplish this, you set the length of your sentences to a certain number and add the generic `<PAD>` token to fill all the empty spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sm2P8y7zNgdU",
    "outputId": "1f1d077a-7c57-42df-fb67-48dd00da39ca",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab[\"the\"]: 9\n",
      "padded token: 35180\n"
     ]
    }
   ],
   "source": [
    "# vocab translates from a word to a unique number\n",
    "print('vocab[\"the\"]:', vocab[\"the\"])\n",
    "# Pad token\n",
    "print('padded token:', vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IY6BTBjunCt1"
   },
   "source": [
    "The tag_map corresponds to one of the possible tags a word can have. Run the cell below to see the possible classes you will be predicting. The prepositions in the tags mean:\n",
    "* I: Token is inside an entity.\n",
    "* B: Token begins an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ZzMamaPcQXWP",
    "outputId": "4f04364c-a88c-4e77-f8bb-9bfcb422d5e9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-geo': 1, 'B-gpe': 2, 'B-per': 3, 'I-geo': 4, 'B-org': 5, 'I-org': 6, 'B-tim': 7, 'B-art': 8, 'I-art': 9, 'I-per': 10, 'I-gpe': 11, 'I-tim': 12, 'B-nat': 13, 'B-eve': 14, 'I-eve': 15, 'I-nat': 16}\n"
     ]
    }
   ],
   "source": [
    "print(tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3F1sUP_MnCt5"
   },
   "source": [
    "So the coding scheme that tags the entities is a minimal one where B- indicates the first token in a multi-token entity, and I- indicates one in the middle of a multi-token entity. If you had the sentence \n",
    "\n",
    "**\"Sharon flew to Miami on Friday\"**\n",
    "\n",
    "the outputs would look like:\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "your tags would reflect three tokens beginning with B-, since there are no multi-token entities in the sequence. But if you added Sharon's last name to the sentence: \n",
    "\n",
    "**\"Sharon Floyd flew to Miami on Friday\"**\n",
    "\n",
    "```\n",
    "Sharon B-per\n",
    "Floyd  I-per\n",
    "flew   O\n",
    "to     O\n",
    "Miami  B-geo\n",
    "on     O\n",
    "Friday B-tim\n",
    "```\n",
    "\n",
    "then your tags would change to show first \"Sharon\" as B-per, and \"Floyd\" as I-per, where I- indicates an inner token in a multi-token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "xM9B_Rwxd01i",
    "outputId": "db098ed6-4351-41f7-cfdb-e45dd3798ebf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of outputs is tag_map 17\n",
      "Num of vocabulary words: 35181\n",
      "The vocab size is 35181\n",
      "The training size is 33570\n",
      "The validation size is 7194\n",
      "An example of the first sentence is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 1, 16, 17, 18, 19, 20, 21]\n",
      "An example of its corresponding label is [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Exploring information about the data\n",
    "print('The number of outputs is tag_map', len(tag_map))\n",
    "# The number of vocabulary tokens (including <PAD>)\n",
    "g_vocab_size = len(vocab)\n",
    "print(f\"Num of vocabulary words: {g_vocab_size}\")\n",
    "print('The vocab size is', len(vocab))\n",
    "print('The training size is', t_size)\n",
    "print('The validation size is', v_size)\n",
    "print('An example of the first sentence is', t_sentences[0])\n",
    "print('An example of its corresponding label is', t_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPd5a-4_nCt8"
   },
   "source": [
    "So you can see that we have already encoded each sentence into a tensor by converting it into a number. We also have 16 possible classes, as shown in the tag map.\n",
    "\n",
    "\n",
    "<a name=\"1.2\"></a>\n",
    "## 1.2  Data generator\n",
    "\n",
    "In python, a generator is a function that behaves like an iterator. It will return the next item. Here is a [link](https://wiki.python.org/moin/Generators) to review python generators. \n",
    "\n",
    "In many AI applications it is very useful to have a data generator. You will now implement a data generator for our NER application.\n",
    "\n",
    "<a name=\"ex01\"></a>\n",
    "### Exercise 01\n",
    "\n",
    "**Instructions:** Implement a data generator function that takes in `batch_size, x, y, pad, shuffle` where x is a large list of sentences, and y is a list of the tags associated with those sentences and pad is a pad value. Return a subset of those inputs in a tuple of two arrays `(X,Y)`. Each is an array of dimension (`batch_size, max_len`), where `max_len` is the length of the longest sentence *in that batch*. You will pad the X and Y examples with the pad argument. If `shuffle=True`, the data will be traversed in a random form.\n",
    "\n",
    "**Details:**\n",
    "\n",
    "This code as an outer loop  \n",
    "```\n",
    "while True:  \n",
    "...  \n",
    "yield((X,Y))  \n",
    "```\n",
    "\n",
    "Which runs continuously in the fashion of generators, pausing when yielding the next values. We will generate a batch_size output on each pass of this loop.    \n",
    "\n",
    "It has two inner loops. \n",
    "1. The first stores in temporal lists the data samples to be included in the next batch, and finds the maximum length of the sentences contained in it. By adjusting the length to include only the size of the longest sentence in each batch, overall computation is reduced. \n",
    "\n",
    "2. The second loop moves those inputs from the temporal list into NumPy arrays pre-filled with pad values.\n",
    "\n",
    "There are three slightly out of the ordinary features. \n",
    "1. The first is the use of the NumPy `full` function to fill the NumPy arrays with a pad value. See [full function documentation](https://numpy.org/doc/1.18/reference/generated/numpy.full.html).\n",
    "\n",
    "2. The second is tracking the current location in the incoming lists of sentences. Generators variables hold their values between invocations, so we create an `index` variable, initialize to zero, and increment by one for each sample included in a batch. However, we do not use the `index` to access the positions of the list of sentences directly. Instead, we use it to select one index from a list of indexes. In this way, we can change the order in which we traverse our original list, keeping untouched our original list.  \n",
    "\n",
    "3. The third also relates to wrapping. Because `batch_size` and the length of the input lists are not aligned, gathering a batch_size group of inputs may involve wrapping back to the beginning of the input loop. In our approach, it is just enough to reset the `index` to 0. We can re-shuffle the list of indexes to produce different batches each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tP7zQC8knCt_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: data_generator\n",
    "def data_generator(batch_size, x, y, pad, shuffle=False, verbose=False):\n",
    "    '''\n",
    "      Input: \n",
    "        batch_size - integer describing the batch size\n",
    "        x - list containing sentences where words are represented as integers\n",
    "        y - list containing tags associated with the sentences\n",
    "        shuffle - Shuffle the data order\n",
    "        pad - an integer representing a pad character\n",
    "        verbose - Print information during runtime\n",
    "      Output:\n",
    "        a tuple containing 2 elements:\n",
    "        X - np.ndarray of dim (batch_size, max_len) of padded sentences\n",
    "        Y - np.ndarray of dim (batch_size, max_len) of tags associated with the sentences in X\n",
    "    '''\n",
    "    \n",
    "    # count the number of lines in data_lines\n",
    "    num_lines = len(x)\n",
    "    \n",
    "    # create an array with the indexes of data_lines that can be shuffled\n",
    "    lines_index = [*range(num_lines)]\n",
    "    \n",
    "    # shuffle the indexes if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(lines_index)\n",
    "    \n",
    "    index = 0 # tracks current location in x, y\n",
    "    while True:\n",
    "        buffer_x = [0] * batch_size # Temporal array to store the raw x data for this batch\n",
    "        buffer_y = [0] * batch_size # Temporal array to store the raw y data for this batch\n",
    "                \n",
    "  ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        \n",
    "        # Copy into the temporal buffers the sentences in x[index : index + batch_size] \n",
    "        # along with their corresponding labels y[index : index + batch_size]\n",
    "        # Find maximum length of sentences in x[index : index + batch_size] for this batch. \n",
    "        # Reset the index if we reach the end of the data set, and shuffle the indexes if needed.\n",
    "        max_len = 0\n",
    "        for i in range(batch_size):\n",
    "             # if the index is greater than or equal to the number of lines in x\n",
    "            if index >= num_lines:\n",
    "                # then reset the index to 0\n",
    "                index = 0\n",
    "                # re-shuffle the indexes if shuffle is set to True\n",
    "                if shuffle:\n",
    "                    rnd.shuffle(lines_index)\n",
    "            \n",
    "            # The current position is obtained using `lines_index[index]`\n",
    "            # Store the x value at the current position into the buffer_x\n",
    "            buffer_x[i] = x[lines_index[index]]\n",
    "            \n",
    "            # Store the y value at the current position into the buffer_y\n",
    "            buffer_y[i] = y[lines_index[index]]\n",
    "            \n",
    "            lenx =  len(buffer_x[i])   #length of current x[]\n",
    "            if lenx > max_len:\n",
    "                max_len = lenx                   #max_len tracks longest x[]\n",
    "            \n",
    "            # increment index by one\n",
    "            index += 1\n",
    "\n",
    "\n",
    "        # create X,Y, NumPy arrays of size (batch_size, max_len) 'full' of pad value\n",
    "        X = np.full((batch_size, max_len), pad)\n",
    "        Y = np.full((batch_size, max_len), pad)\n",
    "\n",
    "        # copy values from lists to NumPy arrays. Use the buffered values\n",
    "        for i in range(batch_size):\n",
    "            # get the example (sentence as a tensor)\n",
    "            # in `buffer_x` at the `i` index\n",
    "            x_i = buffer_x[i]\n",
    "            \n",
    "            # similarly, get the example's labels\n",
    "            # in `buffer_y` at the `i` index\n",
    "            y_i = buffer_y[i]\n",
    "            \n",
    "            # Walk through each word in x_i\n",
    "            for j in range(len(x_i)):\n",
    "                # store the word in x_i at position j into X\n",
    "                X[i, j] = x_i[j]\n",
    "                \n",
    "                # store the label in y_i at position j into Y\n",
    "                Y[i, j] = y_i[j]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "        if verbose: print(\"index=\", index)\n",
    "        yield((X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "s3fwE3PMhOW4",
    "outputId": "12022225-1498-4acb-c0c8-cd02b71c091d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index= 5\n",
      "index= 2\n",
      "(5, 30) (5, 30) (5, 30) (5, 30)\n",
      "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
      "    12    13    14     9    15     1    16    17    18    19    20    21\n",
      " 35180 35180 35180 35180 35180 35180] \n",
      " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
      "     1     0     0     0     0     0     2     0     0     0     0     0\n",
      " 35180 35180 35180 35180 35180 35180]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "mini_sentences = t_sentences[0: 8]\n",
    "mini_labels = t_labels[0: 8]\n",
    "dg = data_generator(batch_size, mini_sentences, mini_labels, vocab[\"<PAD>\"], shuffle=False, verbose=True)\n",
    "X1, Y1 = next(dg)\n",
    "X2, Y2 = next(dg)\n",
    "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n",
    "print(X1[0][:], \"\\n\", Y1[0][:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W-qWOhFunCuH"
   },
   "source": [
    "**Expected output:**   \n",
    "```\n",
    "index= 5\n",
    "index= 2\n",
    "(5, 30) (5, 30) (5, 30) (5, 30)\n",
    "[    0     1     2     3     4     5     6     7     8     9    10    11\n",
    "    12    13    14     9    15     1    16    17    18    19    20    21\n",
    " 35180 35180 35180 35180 35180 35180] \n",
    " [    0     0     0     0     0     0     1     0     0     0     0     0\n",
    "     1     0     0     0     0     0     2     0     0     0     0     0\n",
    " 35180 35180 35180 35180 35180 35180]  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SWxKhkVLr3P"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "# Part 2:  Building the model\n",
    "\n",
    "You will now implement the model. You will be using Google's TensorFlow. Your model will be able to distinguish the following:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "<img src = 'ner1.png' width=\"width\" height=\"height\" style=\"width:500px;height:150px;\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The model architecture will be as follows: \n",
    "\n",
    "<img src = 'ner2.png' width=\"width\" height=\"height\" style=\"width:600px;height:250px;\"/>\n",
    "\n",
    "Concretely: \n",
    "\n",
    "* Use the input tensors you built in your data generator\n",
    "* Feed it into an Embedding layer, to produce more semantic entries\n",
    "* Feed it into an LSTM layer\n",
    "* Run the output through a linear layer\n",
    "* Run the result through a log softmax layer to get the predicted class for each word.\n",
    "\n",
    "Good news! We won't make you implement the LSTM unit drawn above. However, we will ask you to build the model. \n",
    "\n",
    "<a name=\"ex02\"></a>\n",
    "### Exercise 02\n",
    "\n",
    "**Instructions:** Implement the initialization step and the forward function of your Named Entity Recognition system.  \n",
    "Please utilize help function e.g. `help(tl.Dense)` for more information on a layer\n",
    "   \n",
    "- [tl.Serial](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py#L26): Combinator that applies layers serially (by function composition).\n",
    "    - You can pass in the layers as arguments to `Serial`, separated by commas. \n",
    "    - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))` \n",
    "\n",
    "\n",
    "-  [tl.Embedding](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113): Initializes the embedding. In this case it is the dimension of the model by the size of the vocabulary. \n",
    "    - `tl.Embedding(vocab_size, d_feature)`.\n",
    "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
    "    - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n",
    "    \n",
    "\n",
    "-  [tl.LSTM](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py#L87):`Trax` LSTM layer of size d_model. \n",
    "    - `LSTM(n_units)` Builds an LSTM layer of n_cells.\n",
    "\n",
    "\n",
    "\n",
    "-  [tl.Dense](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L28):  A dense layer.\n",
    "    - `tl.Dense(n_units)`: The parameter `n_units` is the number of units chosen for this dense layer.  \n",
    "\n",
    "\n",
    "- [tl.LogSoftmax](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242): Log of the output probabilities.\n",
    "    - Here, you don't need to set any parameters for `LogSoftMax()`.\n",
    " \n",
    "\n",
    "**Online documentation**\n",
    "\n",
    "- [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators)\n",
    "\n",
    "- [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n",
    "\n",
    "-  [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM)\n",
    "\n",
    "-  [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)\n",
    "\n",
    "- [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tag_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-gpe': 2,\n",
       " 'B-per': 3,\n",
       " 'I-geo': 4,\n",
       " 'B-org': 5,\n",
       " 'I-org': 6,\n",
       " 'B-tim': 7,\n",
       " 'B-art': 8,\n",
       " 'I-art': 9,\n",
       " 'I-per': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-tim': 12,\n",
       " 'B-nat': 13,\n",
       " 'B-eve': 14,\n",
       " 'I-eve': 15,\n",
       " 'I-nat': 16}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vL5u72u8Lr3Q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: NER\n",
    "def NER(vocab_size=35181, d_model=50, tags=tag_map):\n",
    "    '''\n",
    "      Input: \n",
    "        vocab_size - integer containing the size of the vocabulary\n",
    "        d_model - integer describing the embedding size\n",
    "      Output:\n",
    "        model - a trax serial model\n",
    "    '''\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    model = tl.Serial(\n",
    "      tl.Embedding(vocab_size, d_model), # Embedding layer\n",
    "      tl.LSTM(d_model), # LSTM layer\n",
    "      tl.Dense(len(tags)), # Dense layer with len(tags) units\n",
    "      tl.LogSoftmax()  # LogSoftmax layer\n",
    "      )\n",
    "      ### END CODE HERE ###\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "BrGdYpPvLr3U",
    "outputId": "3a3e721c-8e63-40ac-f8fa-e500f0abdad5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Embedding_35181_50\n",
      "  LSTM_17\n",
      "  Dense_17\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# initializing your model\n",
    "model = NER()\n",
    "# display your model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p636VCSanCuS"
   },
   "source": [
    "**Expected output:**  \n",
    "```\n",
    "Serial[\n",
    "  Embedding_35181_50\n",
    "  LSTM_50\n",
    "  Dense_17\n",
    "  LogSoftmax\n",
    "]\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LkjXxxhLr3Z"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "# Part 3:  Train the Model \n",
    "\n",
    "This section will train your model.\n",
    "\n",
    "Before you start, you need to create the data generators for training and validation data. It is important that you mask padding in the loss weights of your data, which can be done using the `id_to_mask` argument of `trax.supervised.inputs.add_loss_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lPBR1YrRmEAH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "from trax.data.inputs import add_loss_weights\n",
    "rnd.seed(33)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create training data, mask pad id=35180 for training.\n",
    "train_generator = add_loss_weights(\n",
    "    data_generator(batch_size, t_sentences, t_labels, vocab['<PAD>'], True),\n",
    "    id_to_mask=vocab['<PAD>'])\n",
    "\n",
    "# Create validation data, mask pad id=35180 for training.\n",
    "eval_generator = add_loss_weights(\n",
    "    data_generator(batch_size, v_sentences, v_labels, vocab['<PAD>'], True),\n",
    "    id_to_mask=vocab['<PAD>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35180"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<PAD>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SdkBrFVnCuV"
   },
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 Training the model\n",
    "\n",
    "You will now write a function that takes in your model and trains it.\n",
    "\n",
    "As you've seen in the previous assignments, you will first create the [TrainTask](https://trax-ml.readthedocs.io/en/stable/trax.supervised.html#trax.supervised.training.TrainTask) and [EvalTask](https://trax-ml.readthedocs.io/en/stable/trax.supervised.html#trax.supervised.training.EvalTask) using your data generator. Then you will use the `training.Loop` to train your model.\n",
    "\n",
    "<a name=\"ex03\"></a>\n",
    "### Exercise 03\n",
    "\n",
    "**Instructions:** Implement the `train_model` program below to train the neural network above. Here is a list of things you should do: \n",
    "- Create the trainer object by calling [`trax.supervised.training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) and pass in the following:\n",
    "\n",
    "    - model = [NER](#ex02)\n",
    "    - [training task](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) that uses the train data generator defined in the cell above\n",
    "        - loss_layer = [tl.CrossEntropyLoss()](https://github.com/google/trax/blob/22765bb18608d376d8cd660f9865760e4ff489cd/trax/layers/metrics.py#L71)\n",
    "        - optimizer = [trax.optimizers.Adam(0.01)](https://github.com/google/trax/blob/03cb32995e83fc1455b0c8d1c81a14e894d0b7e3/trax/optimizers/adam.py#L23)\n",
    "    - [evaluation task](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) that uses the validation data generator defined in the cell above\n",
    "        - metrics for `EvalTask`: `tl.CrossEntropyLoss()` and `tl.Accuracy()`\n",
    "        - in `EvalTask` set `n_eval_batches=10` for better evaluation accuracy\n",
    "    - output_dir = output_dir\n",
    "\n",
    "You'll be using a [cross entropy loss](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss), with an [Adam optimizer](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam). Please read the [trax](https://trax-ml.readthedocs.io/en/latest/trax.html) documentation to get a full understanding. The [trax GitHub](https://github.com/google/trax) also contains some useful information and a link to a colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WV27PerULr3a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(NER, train_generator, eval_generator, train_steps=1, output_dir='model'):\n",
    "    '''\n",
    "    Input: \n",
    "        NER - the model you are building\n",
    "        train_generator - The data generator for training examples\n",
    "        eval_generator - The data generator for validation examples,\n",
    "        train_steps - number of training steps\n",
    "        output_dir - folder to save your model\n",
    "    Output:\n",
    "        training_loop - a trax supervised training Loop\n",
    "    '''\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    train_task = training.TrainTask(\n",
    "      train_generator, # A train data generator\n",
    "      loss_layer = tl.CrossEntropyLoss(), # A cross-entropy loss function\n",
    "      optimizer = trax.optimizers.adam.Adam(),\n",
    "      n_steps_per_checkpoint=1  # The adam optimizer\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "      labeled_data = eval_generator, # A labeled data generator\n",
    "      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()], # Evaluate with cross-entropy loss and accuracy\n",
    "      n_eval_batches = 10 # Number of batches to use on each evaluation\n",
    "    )\n",
    "\n",
    "    training_loop = training.Loop(\n",
    "        model=NER, # A model to train\n",
    "        tasks=train_task, # A train task\n",
    "        eval_tasks = eval_task, # The evaluation task\n",
    "        output_dir = output_dir, # The output directory\n",
    "#         checkpoint_at = 5\n",
    "    ) \n",
    "\n",
    "    # Train with train_steps\n",
    "    training_loop.run(n_steps = train_steps)\n",
    "    ### END CODE HERE ###\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35181"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4tIc4nuonCue"
   },
   "source": [
    "On your local machine, you can run this training for 1000 train_steps and get your own model. This training takes about 5 to 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "VU-j8hs-nCue",
    "outputId": "fbbbda7d-b6dd-42e4-a4c6-58c0a6e349b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 1780117\n",
      "Step      1: Ran 1 train steps in 2.03 secs\n",
      "Step      1: train CrossEntropyLoss |  2.29588103\n",
      "Step      1: eval  CrossEntropyLoss |  2.28549259\n",
      "Step      1: eval          Accuracy |  0.02006558\n",
      "\n",
      "Step      2: Ran 1 train steps in 2.02 secs\n",
      "Step      2: train CrossEntropyLoss |  2.31350780\n",
      "Step      2: eval  CrossEntropyLoss |  2.27194693\n",
      "Step      2: eval          Accuracy |  0.02005579\n",
      "\n",
      "Step      3: Ran 1 train steps in 1.94 secs\n",
      "Step      3: train CrossEntropyLoss |  2.27076125\n",
      "Step      3: eval  CrossEntropyLoss |  2.27061327\n",
      "Step      3: eval          Accuracy |  0.02110309\n",
      "\n",
      "Step      4: Ran 1 train steps in 2.05 secs\n",
      "Step      4: train CrossEntropyLoss |  2.26680183\n",
      "Step      4: eval  CrossEntropyLoss |  2.25404518\n",
      "Step      4: eval          Accuracy |  0.01938189\n",
      "\n",
      "Step      5: Ran 1 train steps in 2.07 secs\n",
      "Step      5: train CrossEntropyLoss |  2.25913405\n",
      "Step      5: eval  CrossEntropyLoss |  2.24660008\n",
      "Step      5: eval          Accuracy |  0.01985976\n",
      "\n",
      "Step      6: Ran 1 train steps in 2.30 secs\n",
      "Step      6: train CrossEntropyLoss |  2.25167465\n",
      "Step      6: eval  CrossEntropyLoss |  2.23064699\n",
      "Step      6: eval          Accuracy |  0.01915142\n",
      "\n",
      "Step      7: Ran 1 train steps in 2.04 secs\n",
      "Step      7: train CrossEntropyLoss |  2.23368526\n",
      "Step      7: eval  CrossEntropyLoss |  2.22294955\n",
      "Step      7: eval          Accuracy |  0.02005807\n",
      "\n",
      "Step      8: Ran 1 train steps in 2.31 secs\n",
      "Step      8: train CrossEntropyLoss |  2.22365928\n",
      "Step      8: eval  CrossEntropyLoss |  2.21395082\n",
      "Step      8: eval          Accuracy |  0.01929758\n",
      "\n",
      "Step      9: Ran 1 train steps in 2.38 secs\n",
      "Step      9: train CrossEntropyLoss |  2.20054817\n",
      "Step      9: eval  CrossEntropyLoss |  2.19617522\n",
      "Step      9: eval          Accuracy |  0.02016717\n",
      "\n",
      "Step     10: Ran 1 train steps in 0.39 secs\n",
      "Step     10: train CrossEntropyLoss |  2.21587443\n",
      "Step     10: eval  CrossEntropyLoss |  2.19099870\n",
      "Step     10: eval          Accuracy |  0.01900964\n",
      "\n",
      "Step     11: Ran 1 train steps in 0.40 secs\n",
      "Step     11: train CrossEntropyLoss |  2.19222474\n",
      "Step     11: eval  CrossEntropyLoss |  2.18285668\n",
      "Step     11: eval          Accuracy |  0.02156001\n",
      "\n",
      "Step     12: Ran 1 train steps in 2.68 secs\n",
      "Step     12: train CrossEntropyLoss |  2.19629955\n",
      "Step     12: eval  CrossEntropyLoss |  2.16729760\n",
      "Step     12: eval          Accuracy |  0.01779334\n",
      "\n",
      "Step     13: Ran 1 train steps in 0.41 secs\n",
      "Step     13: train CrossEntropyLoss |  2.19548488\n",
      "Step     13: eval  CrossEntropyLoss |  2.15960877\n",
      "Step     13: eval          Accuracy |  0.02234191\n",
      "\n",
      "Step     14: Ran 1 train steps in 2.09 secs\n",
      "Step     14: train CrossEntropyLoss |  2.15813518\n",
      "Step     14: eval  CrossEntropyLoss |  2.14549198\n",
      "Step     14: eval          Accuracy |  0.02006550\n",
      "\n",
      "Step     15: Ran 1 train steps in 0.40 secs\n",
      "Step     15: train CrossEntropyLoss |  2.15516734\n",
      "Step     15: eval  CrossEntropyLoss |  2.13104966\n",
      "Step     15: eval          Accuracy |  0.01955056\n",
      "\n",
      "Step     16: Ran 1 train steps in 0.41 secs\n",
      "Step     16: train CrossEntropyLoss |  2.13723421\n",
      "Step     16: eval  CrossEntropyLoss |  2.12545283\n",
      "Step     16: eval          Accuracy |  0.01833422\n",
      "\n",
      "Step     17: Ran 1 train steps in 0.43 secs\n",
      "Step     17: train CrossEntropyLoss |  2.12875032\n",
      "Step     17: eval  CrossEntropyLoss |  2.10642855\n",
      "Step     17: eval          Accuracy |  0.02098488\n",
      "\n",
      "Step     18: Ran 1 train steps in 2.02 secs\n",
      "Step     18: train CrossEntropyLoss |  2.10510707\n",
      "Step     18: eval  CrossEntropyLoss |  2.10275414\n",
      "Step     18: eval          Accuracy |  0.01946074\n",
      "\n",
      "Step     19: Ran 1 train steps in 0.43 secs\n",
      "Step     19: train CrossEntropyLoss |  2.10984206\n",
      "Step     19: eval  CrossEntropyLoss |  2.09587636\n",
      "Step     19: eval          Accuracy |  0.01924752\n",
      "\n",
      "Step     20: Ran 1 train steps in 2.02 secs\n",
      "Step     20: train CrossEntropyLoss |  2.09867477\n",
      "Step     20: eval  CrossEntropyLoss |  2.07796357\n",
      "Step     20: eval          Accuracy |  0.01909662\n",
      "\n",
      "Step     21: Ran 1 train steps in 0.43 secs\n",
      "Step     21: train CrossEntropyLoss |  2.05755615\n",
      "Step     21: eval  CrossEntropyLoss |  2.06706491\n",
      "Step     21: eval          Accuracy |  0.02102132\n",
      "\n",
      "Step     22: Ran 1 train steps in 2.26 secs\n",
      "Step     22: train CrossEntropyLoss |  2.06651187\n",
      "Step     22: eval  CrossEntropyLoss |  2.05423207\n",
      "Step     22: eval          Accuracy |  0.01795348\n",
      "\n",
      "Step     23: Ran 1 train steps in 2.18 secs\n",
      "Step     23: train CrossEntropyLoss |  2.03906727\n",
      "Step     23: eval  CrossEntropyLoss |  2.05133893\n",
      "Step     23: eval          Accuracy |  0.01994023\n",
      "\n",
      "Step     24: Ran 1 train steps in 0.41 secs\n",
      "Step     24: train CrossEntropyLoss |  2.03013945\n",
      "Step     24: eval  CrossEntropyLoss |  2.03955607\n",
      "Step     24: eval          Accuracy |  0.02100384\n",
      "\n",
      "Step     25: Ran 1 train steps in 2.07 secs\n",
      "Step     25: train CrossEntropyLoss |  2.03871322\n",
      "Step     25: eval  CrossEntropyLoss |  2.02561426\n",
      "Step     25: eval          Accuracy |  0.02320609\n",
      "\n",
      "Step     26: Ran 1 train steps in 0.42 secs\n",
      "Step     26: train CrossEntropyLoss |  2.01797819\n",
      "Step     26: eval  CrossEntropyLoss |  2.02109332\n",
      "Step     26: eval          Accuracy |  0.02036327\n",
      "\n",
      "Step     27: Ran 1 train steps in 0.41 secs\n",
      "Step     27: train CrossEntropyLoss |  2.01661968\n",
      "Step     27: eval  CrossEntropyLoss |  2.00841585\n",
      "Step     27: eval          Accuracy |  0.04020686\n",
      "\n",
      "Step     28: Ran 1 train steps in 0.42 secs\n",
      "Step     28: train CrossEntropyLoss |  2.03155780\n",
      "Step     28: eval  CrossEntropyLoss |  1.99500436\n",
      "Step     28: eval          Accuracy |  0.08343967\n",
      "\n",
      "Step     29: Ran 1 train steps in 1.98 secs\n",
      "Step     29: train CrossEntropyLoss |  2.00400519\n",
      "Step     29: eval  CrossEntropyLoss |  1.98899020\n",
      "Step     29: eval          Accuracy |  0.16527217\n",
      "\n",
      "Step     30: Ran 1 train steps in 0.42 secs\n",
      "Step     30: train CrossEntropyLoss |  1.97528958\n",
      "Step     30: eval  CrossEntropyLoss |  1.96890143\n",
      "Step     30: eval          Accuracy |  0.26813850\n",
      "\n",
      "Step     31: Ran 1 train steps in 0.42 secs\n",
      "Step     31: train CrossEntropyLoss |  1.98303747\n",
      "Step     31: eval  CrossEntropyLoss |  1.96348627\n",
      "Step     31: eval          Accuracy |  0.40502343\n",
      "\n",
      "Step     32: Ran 1 train steps in 0.43 secs\n",
      "Step     32: train CrossEntropyLoss |  1.95243263\n",
      "Step     32: eval  CrossEntropyLoss |  1.95798005\n",
      "Step     32: eval          Accuracy |  0.56039916\n",
      "\n",
      "Step     33: Ran 1 train steps in 2.13 secs\n",
      "Step     33: train CrossEntropyLoss |  1.94924378\n",
      "Step     33: eval  CrossEntropyLoss |  1.94502048\n",
      "Step     33: eval          Accuracy |  0.69460937\n",
      "\n",
      "Step     34: Ran 1 train steps in 0.43 secs\n",
      "Step     34: train CrossEntropyLoss |  1.94785547\n",
      "Step     34: eval  CrossEntropyLoss |  1.93032572\n",
      "Step     34: eval          Accuracy |  0.77068022\n",
      "\n",
      "Step     35: Ran 1 train steps in 0.43 secs\n",
      "Step     35: train CrossEntropyLoss |  1.93478298\n",
      "Step     35: eval  CrossEntropyLoss |  1.91776141\n",
      "Step     35: eval          Accuracy |  0.81396468\n",
      "\n",
      "Step     36: Ran 1 train steps in 0.44 secs\n",
      "Step     36: train CrossEntropyLoss |  1.94088876\n",
      "Step     36: eval  CrossEntropyLoss |  1.90691888\n",
      "Step     36: eval          Accuracy |  0.83178324\n",
      "\n",
      "Step     37: Ran 1 train steps in 0.44 secs\n",
      "Step     37: train CrossEntropyLoss |  1.89464605\n",
      "Step     37: eval  CrossEntropyLoss |  1.91764444\n",
      "Step     37: eval          Accuracy |  0.82672390\n",
      "\n",
      "Step     38: Ran 1 train steps in 0.43 secs\n",
      "Step     38: train CrossEntropyLoss |  1.92405236\n",
      "Step     38: eval  CrossEntropyLoss |  1.89439816\n",
      "Step     38: eval          Accuracy |  0.83647271\n",
      "\n",
      "Step     39: Ran 1 train steps in 0.44 secs\n",
      "Step     39: train CrossEntropyLoss |  1.88895166\n",
      "Step     39: eval  CrossEntropyLoss |  1.88698188\n",
      "Step     39: eval          Accuracy |  0.83852216\n",
      "\n",
      "Step     40: Ran 1 train steps in 0.43 secs\n",
      "Step     40: train CrossEntropyLoss |  1.88190579\n",
      "Step     40: eval  CrossEntropyLoss |  1.87167555\n",
      "Step     40: eval          Accuracy |  0.84104347\n",
      "\n",
      "Step     41: Ran 1 train steps in 0.44 secs\n",
      "Step     41: train CrossEntropyLoss |  1.88812792\n",
      "Step     41: eval  CrossEntropyLoss |  1.86834919\n",
      "Step     41: eval          Accuracy |  0.84170874\n",
      "\n",
      "Step     42: Ran 1 train steps in 0.44 secs\n",
      "Step     42: train CrossEntropyLoss |  1.84496260\n",
      "Step     42: eval  CrossEntropyLoss |  1.85342050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     42: eval          Accuracy |  0.84387240\n",
      "\n",
      "Step     43: Ran 1 train steps in 0.45 secs\n",
      "Step     43: train CrossEntropyLoss |  1.87694061\n",
      "Step     43: eval  CrossEntropyLoss |  1.84433017\n",
      "Step     43: eval          Accuracy |  0.84905267\n",
      "\n",
      "Step     44: Ran 1 train steps in 2.16 secs\n",
      "Step     44: train CrossEntropyLoss |  1.85286462\n",
      "Step     44: eval  CrossEntropyLoss |  1.83291998\n",
      "Step     44: eval          Accuracy |  0.84509271\n",
      "\n",
      "Step     45: Ran 1 train steps in 0.43 secs\n",
      "Step     45: train CrossEntropyLoss |  1.86465061\n",
      "Step     45: eval  CrossEntropyLoss |  1.82196443\n",
      "Step     45: eval          Accuracy |  0.85082217\n",
      "\n",
      "Step     46: Ran 1 train steps in 2.26 secs\n",
      "Step     46: train CrossEntropyLoss |  1.84924054\n",
      "Step     46: eval  CrossEntropyLoss |  1.81646271\n",
      "Step     46: eval          Accuracy |  0.84585363\n",
      "\n",
      "Step     47: Ran 1 train steps in 2.17 secs\n",
      "Step     47: train CrossEntropyLoss |  1.81376803\n",
      "Step     47: eval  CrossEntropyLoss |  1.80088131\n",
      "Step     47: eval          Accuracy |  0.84831630\n",
      "\n",
      "Step     48: Ran 1 train steps in 0.44 secs\n",
      "Step     48: train CrossEntropyLoss |  1.80749142\n",
      "Step     48: eval  CrossEntropyLoss |  1.79955370\n",
      "Step     48: eval          Accuracy |  0.84712937\n",
      "\n",
      "Step     49: Ran 1 train steps in 0.45 secs\n",
      "Step     49: train CrossEntropyLoss |  1.79186440\n",
      "Step     49: eval  CrossEntropyLoss |  1.78364018\n",
      "Step     49: eval          Accuracy |  0.85185303\n",
      "\n",
      "Step     50: Ran 1 train steps in 0.46 secs\n",
      "Step     50: train CrossEntropyLoss |  1.79699409\n",
      "Step     50: eval  CrossEntropyLoss |  1.77979116\n",
      "Step     50: eval          Accuracy |  0.84193907\n",
      "\n",
      "Step     51: Ran 1 train steps in 0.46 secs\n",
      "Step     51: train CrossEntropyLoss |  1.77315521\n",
      "Step     51: eval  CrossEntropyLoss |  1.77020665\n",
      "Step     51: eval          Accuracy |  0.84606566\n",
      "\n",
      "Step     52: Ran 1 train steps in 2.17 secs\n",
      "Step     52: train CrossEntropyLoss |  1.74849629\n",
      "Step     52: eval  CrossEntropyLoss |  1.76589750\n",
      "Step     52: eval          Accuracy |  0.84207431\n",
      "\n",
      "Step     53: Ran 1 train steps in 0.47 secs\n",
      "Step     53: train CrossEntropyLoss |  1.73616791\n",
      "Step     53: eval  CrossEntropyLoss |  1.75178561\n",
      "Step     53: eval          Accuracy |  0.84401017\n",
      "\n",
      "Step     54: Ran 1 train steps in 0.46 secs\n",
      "Step     54: train CrossEntropyLoss |  1.72773886\n",
      "Step     54: eval  CrossEntropyLoss |  1.74612764\n",
      "Step     54: eval          Accuracy |  0.84105818\n",
      "\n",
      "Step     55: Ran 1 train steps in 0.47 secs\n",
      "Step     55: train CrossEntropyLoss |  1.74030340\n",
      "Step     55: eval  CrossEntropyLoss |  1.71870643\n",
      "Step     55: eval          Accuracy |  0.85752088\n",
      "\n",
      "Step     56: Ran 1 train steps in 0.46 secs\n",
      "Step     56: train CrossEntropyLoss |  1.73991871\n",
      "Step     56: eval  CrossEntropyLoss |  1.71535267\n",
      "Step     56: eval          Accuracy |  0.85169810\n",
      "\n",
      "Step     57: Ran 1 train steps in 1.93 secs\n",
      "Step     57: train CrossEntropyLoss |  1.72164321\n",
      "Step     57: eval  CrossEntropyLoss |  1.72325721\n",
      "Step     57: eval          Accuracy |  0.84065991\n",
      "\n",
      "Step     58: Ran 1 train steps in 0.46 secs\n",
      "Step     58: train CrossEntropyLoss |  1.72360039\n",
      "Step     58: eval  CrossEntropyLoss |  1.70694338\n",
      "Step     58: eval          Accuracy |  0.84490919\n",
      "\n",
      "Step     59: Ran 1 train steps in 0.46 secs\n",
      "Step     59: train CrossEntropyLoss |  1.70869112\n",
      "Step     59: eval  CrossEntropyLoss |  1.69229317\n",
      "Step     59: eval          Accuracy |  0.84931875\n",
      "\n",
      "Step     60: Ran 1 train steps in 0.46 secs\n",
      "Step     60: train CrossEntropyLoss |  1.71891141\n",
      "Step     60: eval  CrossEntropyLoss |  1.68320229\n",
      "Step     60: eval          Accuracy |  0.84768881\n",
      "\n",
      "Step     61: Ran 1 train steps in 0.48 secs\n",
      "Step     61: train CrossEntropyLoss |  1.68203139\n",
      "Step     61: eval  CrossEntropyLoss |  1.67442099\n",
      "Step     61: eval          Accuracy |  0.85067139\n",
      "\n",
      "Step     62: Ran 1 train steps in 0.49 secs\n",
      "Step     62: train CrossEntropyLoss |  1.66883028\n",
      "Step     62: eval  CrossEntropyLoss |  1.66532104\n",
      "Step     62: eval          Accuracy |  0.85075796\n",
      "\n",
      "Step     63: Ran 1 train steps in 2.03 secs\n",
      "Step     63: train CrossEntropyLoss |  1.68734705\n",
      "Step     63: eval  CrossEntropyLoss |  1.64597636\n",
      "Step     63: eval          Accuracy |  0.85660056\n",
      "\n",
      "Step     64: Ran 1 train steps in 0.49 secs\n",
      "Step     64: train CrossEntropyLoss |  1.64862025\n",
      "Step     64: eval  CrossEntropyLoss |  1.64248058\n",
      "Step     64: eval          Accuracy |  0.85291348\n",
      "\n",
      "Step     65: Ran 1 train steps in 0.49 secs\n",
      "Step     65: train CrossEntropyLoss |  1.62174082\n",
      "Step     65: eval  CrossEntropyLoss |  1.63914262\n",
      "Step     65: eval          Accuracy |  0.84910535\n",
      "\n",
      "Step     66: Ran 1 train steps in 0.46 secs\n",
      "Step     66: train CrossEntropyLoss |  1.65453720\n",
      "Step     66: eval  CrossEntropyLoss |  1.64099425\n",
      "Step     66: eval          Accuracy |  0.84252113\n",
      "\n",
      "Step     67: Ran 1 train steps in 0.46 secs\n",
      "Step     67: train CrossEntropyLoss |  1.62406933\n",
      "Step     67: eval  CrossEntropyLoss |  1.62620033\n",
      "Step     67: eval          Accuracy |  0.84776398\n",
      "\n",
      "Step     68: Ran 1 train steps in 0.46 secs\n",
      "Step     68: train CrossEntropyLoss |  1.65584135\n",
      "Step     68: eval  CrossEntropyLoss |  1.63057108\n",
      "Step     68: eval          Accuracy |  0.83750413\n",
      "\n",
      "Step     69: Ran 1 train steps in 0.46 secs\n",
      "Step     69: train CrossEntropyLoss |  1.62177908\n",
      "Step     69: eval  CrossEntropyLoss |  1.61036673\n",
      "Step     69: eval          Accuracy |  0.84468562\n",
      "\n",
      "Step     70: Ran 1 train steps in 2.60 secs\n",
      "Step     70: train CrossEntropyLoss |  1.60953927\n",
      "Step     70: eval  CrossEntropyLoss |  1.59395047\n",
      "Step     70: eval          Accuracy |  0.84929068\n",
      "\n",
      "Step     71: Ran 1 train steps in 0.50 secs\n",
      "Step     71: train CrossEntropyLoss |  1.58576465\n",
      "Step     71: eval  CrossEntropyLoss |  1.58601984\n",
      "Step     71: eval          Accuracy |  0.85112004\n",
      "\n",
      "Step     72: Ran 1 train steps in 0.47 secs\n",
      "Step     72: train CrossEntropyLoss |  1.59056091\n",
      "Step     72: eval  CrossEntropyLoss |  1.58293514\n",
      "Step     72: eval          Accuracy |  0.84538463\n",
      "\n",
      "Step     73: Ran 1 train steps in 0.48 secs\n",
      "Step     73: train CrossEntropyLoss |  1.60103750\n",
      "Step     73: eval  CrossEntropyLoss |  1.58350750\n",
      "Step     73: eval          Accuracy |  0.84079613\n",
      "\n",
      "Step     74: Ran 1 train steps in 2.12 secs\n",
      "Step     74: train CrossEntropyLoss |  1.59443641\n",
      "Step     74: eval  CrossEntropyLoss |  1.56140786\n",
      "Step     74: eval          Accuracy |  0.84804056\n",
      "\n",
      "Step     75: Ran 1 train steps in 0.51 secs\n",
      "Step     75: train CrossEntropyLoss |  1.54535472\n",
      "Step     75: eval  CrossEntropyLoss |  1.54846747\n",
      "Step     75: eval          Accuracy |  0.85324580\n",
      "\n",
      "Step     76: Ran 1 train steps in 0.50 secs\n",
      "Step     76: train CrossEntropyLoss |  1.62196553\n",
      "Step     76: eval  CrossEntropyLoss |  1.54729714\n",
      "Step     76: eval          Accuracy |  0.84895779\n",
      "\n",
      "Step     77: Ran 1 train steps in 0.50 secs\n",
      "Step     77: train CrossEntropyLoss |  1.57738996\n",
      "Step     77: eval  CrossEntropyLoss |  1.53934500\n",
      "Step     77: eval          Accuracy |  0.84957777\n",
      "\n",
      "Step     78: Ran 1 train steps in 0.49 secs\n",
      "Step     78: train CrossEntropyLoss |  1.51195252\n",
      "Step     78: eval  CrossEntropyLoss |  1.53386295\n",
      "Step     78: eval          Accuracy |  0.84578717\n",
      "\n",
      "Step     79: Ran 1 train steps in 0.49 secs\n",
      "Step     79: train CrossEntropyLoss |  1.56798267\n",
      "Step     79: eval  CrossEntropyLoss |  1.51783055\n",
      "Step     79: eval          Accuracy |  0.85082108\n",
      "\n",
      "Step     80: Ran 1 train steps in 0.49 secs\n",
      "Step     80: train CrossEntropyLoss |  1.53983152\n",
      "Step     80: eval  CrossEntropyLoss |  1.52799907\n",
      "Step     80: eval          Accuracy |  0.84158890\n",
      "\n",
      "Step     81: Ran 1 train steps in 0.49 secs\n",
      "Step     81: train CrossEntropyLoss |  1.52307260\n",
      "Step     81: eval  CrossEntropyLoss |  1.51832200\n",
      "Step     81: eval          Accuracy |  0.84354361\n",
      "\n",
      "Step     82: Ran 1 train steps in 0.52 secs\n",
      "Step     82: train CrossEntropyLoss |  1.49543047\n",
      "Step     82: eval  CrossEntropyLoss |  1.49903802\n",
      "Step     82: eval          Accuracy |  0.85057429\n",
      "\n",
      "Step     83: Ran 1 train steps in 0.50 secs\n",
      "Step     83: train CrossEntropyLoss |  1.49580479\n",
      "Step     83: eval  CrossEntropyLoss |  1.50641481\n",
      "Step     83: eval          Accuracy |  0.84079983\n",
      "\n",
      "Step     84: Ran 1 train steps in 0.50 secs\n",
      "Step     84: train CrossEntropyLoss |  1.47502089\n",
      "Step     84: eval  CrossEntropyLoss |  1.47900372\n",
      "Step     84: eval          Accuracy |  0.85076879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step     85: Ran 1 train steps in 0.51 secs\n",
      "Step     85: train CrossEntropyLoss |  1.44729090\n",
      "Step     85: eval  CrossEntropyLoss |  1.47811348\n",
      "Step     85: eval          Accuracy |  0.84608399\n",
      "\n",
      "Step     86: Ran 1 train steps in 0.51 secs\n",
      "Step     86: train CrossEntropyLoss |  1.44435227\n",
      "Step     86: eval  CrossEntropyLoss |  1.45631177\n",
      "Step     86: eval          Accuracy |  0.85550435\n",
      "\n",
      "Step     87: Ran 1 train steps in 0.53 secs\n",
      "Step     87: train CrossEntropyLoss |  1.45945203\n",
      "Step     87: eval  CrossEntropyLoss |  1.46198217\n",
      "Step     87: eval          Accuracy |  0.84784312\n",
      "\n",
      "Step     88: Ran 1 train steps in 2.18 secs\n",
      "Step     88: train CrossEntropyLoss |  1.42932582\n",
      "Step     88: eval  CrossEntropyLoss |  1.44967436\n",
      "Step     88: eval          Accuracy |  0.85042083\n",
      "\n",
      "Step     89: Ran 1 train steps in 0.51 secs\n",
      "Step     89: train CrossEntropyLoss |  1.42969704\n",
      "Step     89: eval  CrossEntropyLoss |  1.45475641\n",
      "Step     89: eval          Accuracy |  0.84235274\n",
      "\n",
      "Step     90: Ran 1 train steps in 0.50 secs\n",
      "Step     90: train CrossEntropyLoss |  1.45271862\n",
      "Step     90: eval  CrossEntropyLoss |  1.45524101\n",
      "Step     90: eval          Accuracy |  0.84062688\n",
      "\n",
      "Step     91: Ran 1 train steps in 0.50 secs\n",
      "Step     91: train CrossEntropyLoss |  1.41785872\n",
      "Step     91: eval  CrossEntropyLoss |  1.43878837\n",
      "Step     91: eval          Accuracy |  0.84550242\n",
      "\n",
      "Step     92: Ran 1 train steps in 0.50 secs\n",
      "Step     92: train CrossEntropyLoss |  1.43663681\n",
      "Step     92: eval  CrossEntropyLoss |  1.41971760\n",
      "Step     92: eval          Accuracy |  0.84991733\n",
      "\n",
      "Step     93: Ran 1 train steps in 0.52 secs\n",
      "Step     93: train CrossEntropyLoss |  1.42653811\n",
      "Step     93: eval  CrossEntropyLoss |  1.41870226\n",
      "Step     93: eval          Accuracy |  0.84696591\n",
      "\n",
      "Step     94: Ran 1 train steps in 0.50 secs\n",
      "Step     94: train CrossEntropyLoss |  1.41057575\n",
      "Step     94: eval  CrossEntropyLoss |  1.41068541\n",
      "Step     94: eval          Accuracy |  0.84755343\n",
      "\n",
      "Step     95: Ran 1 train steps in 0.53 secs\n",
      "Step     95: train CrossEntropyLoss |  1.39897120\n",
      "Step     95: eval  CrossEntropyLoss |  1.41035030\n",
      "Step     95: eval          Accuracy |  0.84191053\n",
      "\n",
      "Step     96: Ran 1 train steps in 0.52 secs\n",
      "Step     96: train CrossEntropyLoss |  1.37242925\n",
      "Step     96: eval  CrossEntropyLoss |  1.39779950\n",
      "Step     96: eval          Accuracy |  0.84708443\n",
      "\n",
      "Step     97: Ran 1 train steps in 0.51 secs\n",
      "Step     97: train CrossEntropyLoss |  1.43419075\n",
      "Step     97: eval  CrossEntropyLoss |  1.37892528\n",
      "Step     97: eval          Accuracy |  0.85169545\n",
      "\n",
      "Step     98: Ran 1 train steps in 0.52 secs\n",
      "Step     98: train CrossEntropyLoss |  1.42381620\n",
      "Step     98: eval  CrossEntropyLoss |  1.38206297\n",
      "Step     98: eval          Accuracy |  0.84961169\n",
      "\n",
      "Step     99: Ran 1 train steps in 0.53 secs\n",
      "Step     99: train CrossEntropyLoss |  1.37328398\n",
      "Step     99: eval  CrossEntropyLoss |  1.37789962\n",
      "Step     99: eval          Accuracy |  0.84582877\n",
      "\n",
      "Step    100: Ran 1 train steps in 0.51 secs\n",
      "Step    100: train CrossEntropyLoss |  1.35318208\n",
      "Step    100: eval  CrossEntropyLoss |  1.38285936\n",
      "Step    100: eval          Accuracy |  0.83922676\n"
     ]
    }
   ],
   "source": [
    "train_steps = 10          # In coursera we can only train 100 steps\n",
    "!rm -f 'model/model.pkl.gz'  # Remove old model.pkl if it exists\n",
    "\n",
    "# Train the model\n",
    "training_loop = train_model(NER(), train_generator, eval_generator, train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p1QvV66ZLr3i"
   },
   "source": [
    "**Expected output (Approximately)**\n",
    "\n",
    "```\n",
    "...\n",
    "Step      1: train CrossEntropyLoss |  2.94375849\n",
    "Step      1: eval  CrossEntropyLoss |  1.93172036\n",
    "Step      1: eval          Accuracy |  0.78727312\n",
    "Step    100: train CrossEntropyLoss |  0.57727730\n",
    "Step    100: eval  CrossEntropyLoss |  0.36356260\n",
    "Step    100: eval          Accuracy |  0.90943187\n",
    "...\n",
    "```\n",
    "This value may change between executions, but it must be around 90% of accuracy on train and validations sets, after 100 training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQTurbC0nCuh"
   },
   "source": [
    "We have trained the model longer, and we give you such a trained model. In that way, we ensure you can continue with the rest of the assignment even if you had some troubles up to here, and also we are sure that everybody will get the same outputs for the last example. However, you are free to try your model, as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trax.layers.combinators.Serial"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_loop.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trax.layers.combinators.Serial"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(NER())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[-2.006753  , -2.561296  , -3.013745  , ..., -3.1007216 ,\n",
       "               -3.4194183 , -2.4634278 ],\n",
       "              [-1.5225184 , -2.5695312 , -3.2377772 , ..., -3.3339489 ,\n",
       "               -3.714285  , -2.375781  ],\n",
       "              [-1.2707342 , -2.6294258 , -3.3588178 , ..., -3.4178607 ,\n",
       "               -3.8123758 , -2.4223394 ],\n",
       "              ...,\n",
       "              [-0.9852531 , -2.6778789 , -3.506508  , ..., -3.3992314 ,\n",
       "               -3.906118  , -2.631003  ],\n",
       "              [-0.98525333, -2.6778786 , -3.5065067 , ..., -3.3992321 ,\n",
       "               -3.9061177 , -2.6310022 ],\n",
       "              [-0.98525286, -2.6778786 , -3.5065057 , ..., -3.3992326 ,\n",
       "               -3.9061174 , -2.631002  ]],\n",
       "\n",
       "             [[-2.0605528 , -2.576471  , -3.0074682 , ..., -3.1463857 ,\n",
       "               -3.3743157 , -2.4754627 ],\n",
       "              [-1.5060326 , -2.5896046 , -3.1879435 , ..., -3.282409  ,\n",
       "               -3.6338933 , -2.38158   ],\n",
       "              [-1.2246009 , -2.6504393 , -3.3103724 , ..., -3.353086  ,\n",
       "               -3.8205287 , -2.4702828 ],\n",
       "              ...,\n",
       "              [-0.9852414 , -2.677872  , -3.5067708 , ..., -3.3989909 ,\n",
       "               -3.9061425 , -2.6311305 ],\n",
       "              [-0.98524356, -2.677873  , -3.5067213 , ..., -3.3990366 ,\n",
       "               -3.9061368 , -2.6311042 ],\n",
       "              [-0.98524547, -2.677874  , -3.5066807 , ..., -3.399074  ,\n",
       "               -3.906132  , -2.6310835 ]],\n",
       "\n",
       "             [[-2.061671  , -2.611768  , -2.9625764 , ..., -3.1387007 ,\n",
       "               -3.3174732 , -2.4739268 ],\n",
       "              [-1.5122776 , -2.6008291 , -3.2102854 , ..., -3.3108864 ,\n",
       "               -3.6186597 , -2.3970153 ],\n",
       "              [-1.2695439 , -2.6255429 , -3.3250847 , ..., -3.4152071 ,\n",
       "               -3.7768903 , -2.4132824 ],\n",
       "              ...,\n",
       "              [-0.98525214, -2.677877  , -3.506553  , ..., -3.3991878 ,\n",
       "               -3.9061208 , -2.6310246 ],\n",
       "              [-0.98525214, -2.6778774 , -3.5065434 , ..., -3.399197  ,\n",
       "               -3.9061203 , -2.63102   ],\n",
       "              [-0.98525214, -2.6778777 , -3.5065355 , ..., -3.3992043 ,\n",
       "               -3.9061193 , -2.6310158 ]],\n",
       "\n",
       "             ...,\n",
       "\n",
       "             [[-2.0467596 , -2.562007  , -2.9556205 , ..., -3.1220326 ,\n",
       "               -3.361722  , -2.454319  ],\n",
       "              [-1.4803797 , -2.5458775 , -3.181213  , ..., -3.3286579 ,\n",
       "               -3.7624211 , -2.388321  ],\n",
       "              [-1.2407882 , -2.6548994 , -3.3648992 , ..., -3.3915787 ,\n",
       "               -3.812345  , -2.4565473 ],\n",
       "              ...,\n",
       "              [-0.98525286, -2.6778786 , -3.5065098 , ..., -3.3992295 ,\n",
       "               -3.906118  , -2.6310036 ],\n",
       "              [-0.9852531 , -2.6778786 , -3.5065084 , ..., -3.3992305 ,\n",
       "               -3.906118  , -2.631003  ],\n",
       "              [-0.98525286, -2.6778789 , -3.5065074 , ..., -3.399232  ,\n",
       "               -3.906118  , -2.6310027 ]],\n",
       "\n",
       "             [[-2.0162635 , -2.567471  , -2.9994903 , ..., -3.1166072 ,\n",
       "               -3.3782883 , -2.4886603 ],\n",
       "              [-1.5022776 , -2.577454  , -3.1870546 , ..., -3.2977805 ,\n",
       "               -3.6569314 , -2.4006162 ],\n",
       "              [-1.258521  , -2.622664  , -3.3711028 , ..., -3.3800917 ,\n",
       "               -3.8119545 , -2.439886  ],\n",
       "              ...,\n",
       "              [-0.98525333, -2.6778786 , -3.506503  , ..., -3.3992352 ,\n",
       "               -3.9061177 , -2.6310008 ],\n",
       "              [-0.98525333, -2.6778786 , -3.5065029 , ..., -3.3992357 ,\n",
       "               -3.9061177 , -2.6310008 ],\n",
       "              [-0.9852531 , -2.6778786 , -3.5065029 , ..., -3.3992357 ,\n",
       "               -3.9061177 , -2.6310008 ]],\n",
       "\n",
       "             [[-2.0428858 , -2.586059  , -2.9758823 , ..., -3.1174824 ,\n",
       "               -3.3535311 , -2.4470873 ],\n",
       "              [-1.4531977 , -2.6007552 , -3.202187  , ..., -3.3239799 ,\n",
       "               -3.646916  , -2.4009666 ],\n",
       "              [-1.2417176 , -2.647208  , -3.3226836 , ..., -3.4022    ,\n",
       "               -3.7439818 , -2.423912  ],\n",
       "              ...,\n",
       "              [-0.98525286, -2.6778784 , -3.5065026 , ..., -3.3992355 ,\n",
       "               -3.9061174 , -2.6310005 ],\n",
       "              [-0.9852531 , -2.6778784 , -3.5065026 , ..., -3.3992355 ,\n",
       "               -3.9061174 , -2.6310005 ],\n",
       "              [-0.98525286, -2.6778784 , -3.5065024 , ..., -3.3992355 ,\n",
       "               -3.9061174 , -2.6310005 ]]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "LayerError",
     "evalue": "Exception passing through layer Serial (in pure_fn):\n  layer created in file [...]/<ipython-input-30-4fdd5eddfa43>, line 12\n  layer input shapes: ShapeDtype{shape:(7194, 70), dtype:int64}\n\n  File [...]/trax/layers/combinators.py, line 88, in forward\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\n\nLayerError: Exception passing through layer Embedding_35181_50 (in pure_fn):\n  layer created in file [...]/<ipython-input-30-4fdd5eddfa43>, line 13\n  layer input shapes: ShapeDtype{shape:(7194, 70), dtype:int64}\n\n  File [...]/trax/layers/assert_shape.py, line 122, in forward_wrapper\n    y = forward(self, x, *args, **kwargs)\n\n  File [...]/trax/layers/core.py, line 181, in forward\n    embedded = jnp.take(self.weights, x, axis=0)\n\n  File [...]/_src/numpy/lax_numpy.py, line 4292, in take\n    return lax.gather(a, indices[..., None], dimension_numbers=dnums,\n\n  File [...]/_src/lax/lax.py, line 886, in gather\n    return gather_p.bind(\n\n  File [...]/site-packages/jax/core.py, line 259, in bind\n    out = top_trace.process_primitive(self, tracers, params)\n\n  File [...]/site-packages/jax/core.py, line 597, in process_primitive\n    return primitive.impl(*tracers, **params)\n\n  File [...]/jax/interpreters/xla.py, line 230, in apply_primitive\n    compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args), **params)\n\n  File [...]/jax/_src/util.py, line 197, in wrapper\n    return cached(bool(config.x64_enabled), *args, **kwargs)\n\n  File [...]/jax/_src/util.py, line 190, in cached\n    return f(*args, **kwargs)\n\n  File [...]/jax/interpreters/xla.py, line 255, in xla_primitive_callable\n    aval_out = prim.abstract_eval(*avals, **params)\n\n  File [...]/_src/lax/lax.py, line 2010, in standard_abstract_eval\n    return ShapedArray(shape_rule(*avals, **kwargs), dtype_rule(*avals, **kwargs),\n\n  File [...]/_src/lax/lax.py, line 4268, in _gather_shape_rule\n    raise TypeError(f\"Slice size at index {i} in gather op is out of range, \"\n\nTypeError: Slice size at index 0 in gather op is out of range, must be within [0, 1), got 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLayerError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-3cccf053d175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, weights, state, rng)\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m  \u001b[0;31m# Needed if the model wasn't fully initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    546\u001b[0m       \u001b[0;31m# Skipping 3 lines as it's always the uninteresting internal call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_short_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m       raise LayerError(name, 'pure_fn',\n\u001b[0m\u001b[1;32m    549\u001b[0m                        self._caller, signature(x), trace) from None\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLayerError\u001b[0m: Exception passing through layer Serial (in pure_fn):\n  layer created in file [...]/<ipython-input-30-4fdd5eddfa43>, line 12\n  layer input shapes: ShapeDtype{shape:(7194, 70), dtype:int64}\n\n  File [...]/trax/layers/combinators.py, line 88, in forward\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\n\nLayerError: Exception passing through layer Embedding_35181_50 (in pure_fn):\n  layer created in file [...]/<ipython-input-30-4fdd5eddfa43>, line 13\n  layer input shapes: ShapeDtype{shape:(7194, 70), dtype:int64}\n\n  File [...]/trax/layers/assert_shape.py, line 122, in forward_wrapper\n    y = forward(self, x, *args, **kwargs)\n\n  File [...]/trax/layers/core.py, line 181, in forward\n    embedded = jnp.take(self.weights, x, axis=0)\n\n  File [...]/_src/numpy/lax_numpy.py, line 4292, in take\n    return lax.gather(a, indices[..., None], dimension_numbers=dnums,\n\n  File [...]/_src/lax/lax.py, line 886, in gather\n    return gather_p.bind(\n\n  File [...]/site-packages/jax/core.py, line 259, in bind\n    out = top_trace.process_primitive(self, tracers, params)\n\n  File [...]/site-packages/jax/core.py, line 597, in process_primitive\n    return primitive.impl(*tracers, **params)\n\n  File [...]/jax/interpreters/xla.py, line 230, in apply_primitive\n    compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args), **params)\n\n  File [...]/jax/_src/util.py, line 197, in wrapper\n    return cached(bool(config.x64_enabled), *args, **kwargs)\n\n  File [...]/jax/_src/util.py, line 190, in cached\n    return f(*args, **kwargs)\n\n  File [...]/jax/interpreters/xla.py, line 255, in xla_primitive_callable\n    aval_out = prim.abstract_eval(*avals, **params)\n\n  File [...]/_src/lax/lax.py, line 2010, in standard_abstract_eval\n    return ShapedArray(shape_rule(*avals, **kwargs), dtype_rule(*avals, **kwargs),\n\n  File [...]/_src/lax/lax.py, line 4268, in _gather_shape_rule\n    raise TypeError(f\"Slice size at index {i} in gather op is out of range, \"\n\nTypeError: Slice size at index 0 in gather op is out of range, must be within [0, 1), got 1."
     ]
    }
   ],
   "source": [
    "a = NER()\n",
    "a(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecIG67nenCui",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "model.pkl.gz; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-00b8a74a3082>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the pretrained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.pkl.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trax/layers/base.py\u001b[0m in \u001b[0;36minit_from_file\u001b[0;34m(self, file_name, weights_only, input_signature)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgzipf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzipf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0minput_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_signature'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mpeek\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"peek() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# jump to the next member, if there is one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36m_read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_gzip_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mmagic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmagic\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mregular\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \"\"\"\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m         raise errors.PermissionDeniedError(None, None,\n\u001b[1;32m     78\u001b[0m                                            \"File isn't open for reading\")\n\u001b[0;32m---> 79\u001b[0;31m       self._read_buf = _pywrap_file_io.BufferedInputStream(\n\u001b[0m\u001b[1;32m     80\u001b[0m           compat.path_to_str(self.__name), 1024 * 512)\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: model.pkl.gz; No such file or directory"
     ]
    }
   ],
   "source": [
    "# loading in a pretrained model..\n",
    "model = NER()\n",
    "model.init(trax.shapes.ShapeDtype((1, 1), dtype=np.int32))\n",
    "\n",
    "# Load the pretrained model\n",
    "model.init_from_file('model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c4r-gXOZLr3j"
   },
   "source": [
    "<a name=\"4\"></a>\n",
    "# Part 4:  Compute Accuracy\n",
    "\n",
    "You will now evaluate in the test set. Previously, you have seen the accuracy on the training set and the validation (noted as eval) set. You will now evaluate on your test set. To get a good evaluation, you will need to create a mask to avoid counting the padding tokens when computing the accuracy. \n",
    "\n",
    "<a name=\"ex04\"></a>\n",
    "### Exercise 04\n",
    "\n",
    "**Instructions:** Write a program that takes in your model and uses it to evaluate on the test set. You should be able to get an accuracy of 95%.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmIvd_GXnCuk"
   },
   "source": [
    "\n",
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>More Detailed Instructions </b></font>\n",
    "</summary>\n",
    "\n",
    "* *Step 1*: model(sentences) will give you the predicted output. \n",
    "\n",
    "* *Step 2*: Prediction will produce an output with an added dimension. For each sentence, for each word, there will be a vector of probabilities for each tag type. For each sentence,word, you need to pick the maximum valued tag. This will require `np.argmax` and careful use of the `axis` argument.\n",
    "* *Step 3*: Create a mask to prevent counting pad characters. It has the same dimension as output. An example below on matrix comparison provides a hint.\n",
    "* *Step 4*: Compute the accuracy metric by comparing your outputs against your test labels. Take the sum of that and divide by the total number of **unpadded** tokens. Use your mask value to mask the padded tokens. Return the accuracy. \n",
    "</detail>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qaSy_NywnCul",
    "outputId": "7ceb7d3f-6948-48e0-afd9-003bfe0d0a70",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a comparision on a matrix \n",
    "a = np.array([1, 2, 3, 4])\n",
    "a == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3H0Kx1rnnCun",
    "outputId": "cf3789c6-374e-402b-e833-8a2e5c5d8d33",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shapes (7194, 70) (7194, 70)\n"
     ]
    }
   ],
   "source": [
    "# create the evaluation inputs\n",
    "x, y = next(data_generator(len(test_sentences), test_sentences, test_labels, vocab['<PAD>']))\n",
    "print(\"input shapes\", x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   61,  9699,  3006,   151,   241,  2500,    63,  6327,   250,\n",
       "        5421, 13557,   250,    11,   643,  1011,    93,   397,  5372,\n",
       "         151,   241,  5279,   191,  6649,    34,     9,   293, 11245,\n",
       "           7,  6401, 32666,    34,   134,    35, 13564,    35, 10838,\n",
       "          21, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rh16zSTonCuq",
    "outputId": "26aaa4e9-b34b-4c0a-99e4-35c0af52470b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.DeviceArray'>\n",
      "tmp_pred has shape: (7194, 70, 17)\n"
     ]
    }
   ],
   "source": [
    "# sample prediction\n",
    "tmp_pred = training_loop.model(x)\n",
    "print(type(tmp_pred))\n",
    "print(f\"tmp_pred has shape: {tmp_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0, 0, 0, ..., 0, 0, 0],\n",
       "             [0, 0, 0, ..., 0, 0, 0],\n",
       "             [0, 0, 0, ..., 0, 0, 0],\n",
       "             ...,\n",
       "             [0, 0, 0, ..., 0, 0, 0],\n",
       "             [0, 0, 0, ..., 0, 0, 0],\n",
       "             [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(tmp_pred, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "78l5MTSBnCut"
   },
   "source": [
    "Note that the model's prediction has 3 axes: \n",
    "- the number of examples\n",
    "- the number of words in each example (padded to be as long as the longest sentence in the batch)\n",
    "- the number of possible targets (the 17 named entity tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ek59ro9nCut"
   },
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: evaluate_prediction\n",
    "def evaluate_prediction(pred, labels, pad):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        pred: prediction array with shape \n",
    "            (num examples, max sentence length in batch, num of classes)\n",
    "        labels: array of size (batch_size, seq_len)\n",
    "        pad: integer representing pad character\n",
    "    Outputs:\n",
    "        accuracy: float\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "## step 1 ##\n",
    "    outputs = np.argmax(pred, axis=2)\n",
    "    print(\"outputs shape:\", outputs.shape)\n",
    "\n",
    "## step 2 ##\n",
    "    mask = labels != pad\n",
    "    print(\"mask shape:\", mask.shape, \"mask[0][20:30]:\", mask[0][20:30])\n",
    "## step 3 ##\n",
    "    accuracy = np.sum(outputs == labels) / float(np.sum(mask))\n",
    "    ### END CODE HERE ###\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-2.0552309 , -2.5496306 , -2.940298  , ..., -3.1274855 ,\n",
       "              -3.3596818 , -2.4675417 ],\n",
       "             [-1.481727  , -2.5616913 , -3.2041557 , ..., -3.2991939 ,\n",
       "              -3.6773381 , -2.3824534 ],\n",
       "             [-1.2467663 , -2.6278791 , -3.3506947 , ..., -3.3944504 ,\n",
       "              -3.840014  , -2.474895  ],\n",
       "             ...,\n",
       "             [-0.98525167, -2.677876  , -3.506569  , ..., -3.3991728 ,\n",
       "              -3.9061232 , -2.631033  ],\n",
       "             [-0.98525214, -2.6778762 , -3.5065563 , ..., -3.3991845 ,\n",
       "              -3.9061213 , -2.6310263 ],\n",
       "             [-0.9852524 , -2.677877  , -3.506546  , ..., -3.3991945 ,\n",
       "              -3.9061208 , -2.6310213 ]], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_pred[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     5,     0,     0,     0,     0,     0,     0, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180, 35180,\n",
       "       35180, 35180, 35180, 35180, 35180, 35180, 35180])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "LayerError",
     "evalue": "Exception passing through layer Serial (in pure_fn):\n  layer created in file [...]/<ipython-input-30-4fdd5eddfa43>, line 12\n  layer input shapes: ShapeDtype{shape:(7194, 70), dtype:int64}\n\n  File [...]/trax/layers/combinators.py, line 88, in forward\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\n\nLayerError: Exception passing through layer Embedding_35181_50 (in pure_fn):\n  layer created in file [...]/<ipython-input-30-4fdd5eddfa43>, line 13\n  layer input shapes: ShapeDtype{shape:(7194, 70), dtype:int64}\n\n  File [...]/trax/layers/assert_shape.py, line 122, in forward_wrapper\n    y = forward(self, x, *args, **kwargs)\n\n  File [...]/trax/layers/core.py, line 181, in forward\n    embedded = jnp.take(self.weights, x, axis=0)\n\n  File [...]/_src/numpy/lax_numpy.py, line 4292, in take\n    return lax.gather(a, indices[..., None], dimension_numbers=dnums,\n\n  File [...]/_src/lax/lax.py, line 886, in gather\n    return gather_p.bind(\n\n  File [...]/site-packages/jax/core.py, line 259, in bind\n    out = top_trace.process_primitive(self, tracers, params)\n\n  File [...]/site-packages/jax/core.py, line 597, in process_primitive\n    return primitive.impl(*tracers, **params)\n\n  File [...]/jax/interpreters/xla.py, line 230, in apply_primitive\n    compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args), **params)\n\n  File [...]/jax/_src/util.py, line 197, in wrapper\n    return cached(bool(config.x64_enabled), *args, **kwargs)\n\n  File [...]/jax/_src/util.py, line 190, in cached\n    return f(*args, **kwargs)\n\n  File [...]/jax/interpreters/xla.py, line 255, in xla_primitive_callable\n    aval_out = prim.abstract_eval(*avals, **params)\n\n  File [...]/_src/lax/lax.py, line 2010, in standard_abstract_eval\n    return ShapedArray(shape_rule(*avals, **kwargs), dtype_rule(*avals, **kwargs),\n\n  File [...]/_src/lax/lax.py, line 4268, in _gather_shape_rule\n    raise TypeError(f\"Slice size at index {i} in gather op is out of range, \"\n\nTypeError: Slice size at index 0 in gather op is out of range, must be within [0, 1), got 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLayerError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-658534f5a942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, weights, state, rng)\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m  \u001b[0;31m# Needed if the model wasn't fully initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trax/layers/base.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m    546\u001b[0m       \u001b[0;31m# Skipping 3 lines as it's always the uninteresting internal call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_short_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m       raise LayerError(name, 'pure_fn',\n\u001b[0m\u001b[1;32m    549\u001b[0m                        self._caller, signature(x), trace) from None\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLayerError\u001b[0m: Exception passing through layer Serial (in pure_fn):\n  layer created in file [...]/<ipython-input-30-4fdd5eddfa43>, line 12\n  layer input shapes: ShapeDtype{shape:(7194, 70), dtype:int64}\n\n  File [...]/trax/layers/combinators.py, line 88, in forward\n    outputs, s = layer.pure_fn(inputs, w, s, rng, use_cache=True)\n\nLayerError: Exception passing through layer Embedding_35181_50 (in pure_fn):\n  layer created in file [...]/<ipython-input-30-4fdd5eddfa43>, line 13\n  layer input shapes: ShapeDtype{shape:(7194, 70), dtype:int64}\n\n  File [...]/trax/layers/assert_shape.py, line 122, in forward_wrapper\n    y = forward(self, x, *args, **kwargs)\n\n  File [...]/trax/layers/core.py, line 181, in forward\n    embedded = jnp.take(self.weights, x, axis=0)\n\n  File [...]/_src/numpy/lax_numpy.py, line 4292, in take\n    return lax.gather(a, indices[..., None], dimension_numbers=dnums,\n\n  File [...]/_src/lax/lax.py, line 886, in gather\n    return gather_p.bind(\n\n  File [...]/site-packages/jax/core.py, line 259, in bind\n    out = top_trace.process_primitive(self, tracers, params)\n\n  File [...]/site-packages/jax/core.py, line 597, in process_primitive\n    return primitive.impl(*tracers, **params)\n\n  File [...]/jax/interpreters/xla.py, line 230, in apply_primitive\n    compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args), **params)\n\n  File [...]/jax/_src/util.py, line 197, in wrapper\n    return cached(bool(config.x64_enabled), *args, **kwargs)\n\n  File [...]/jax/_src/util.py, line 190, in cached\n    return f(*args, **kwargs)\n\n  File [...]/jax/interpreters/xla.py, line 255, in xla_primitive_callable\n    aval_out = prim.abstract_eval(*avals, **params)\n\n  File [...]/_src/lax/lax.py, line 2010, in standard_abstract_eval\n    return ShapedArray(shape_rule(*avals, **kwargs), dtype_rule(*avals, **kwargs),\n\n  File [...]/_src/lax/lax.py, line 4268, in _gather_shape_rule\n    raise TypeError(f\"Slice size at index {i} in gather op is out of range, \"\n\nTypeError: Slice size at index 0 in gather op is out of range, must be within [0, 1), got 1."
     ]
    }
   ],
   "source": [
    "a = NER()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial[\n",
       "  Embedding_[[ 4046  3007    18 ... 35180 35180 35180]\n",
       "   [ 3175   347     1 ... 35180 35180 35180]\n",
       "   [  367  1989 11962 ... 35180 35180 35180]\n",
       "   ...\n",
       "   [ 1521   125   172 ... 35180 35180 35180]\n",
       "   [  183   296  1889 ... 35180 35180 35180]\n",
       "   [   42   126   273 ... 35180 35180 35180]]_50\n",
       "  LSTM_50\n",
       "  Dense_17\n",
       "  LogSoftmax\n",
       "]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "yCWFwt3m1sgL",
    "outputId": "701d6b4d-b9b6-41f7-80b3-d0c043880704"
   },
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-0ab65031e052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-1a9c84b55e85>\u001b[0m in \u001b[0;36mevaluate_prediction\u001b[0;34m(pred, labels, pad)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m### START CODE HERE (Replace instances of 'None' with your code) ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m## step 1 ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputs shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \"\"\"\n\u001b[0;32m-> 1188\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_prediction(tmp_pred, y, vocab['<PAD>'])\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-0ab65031e052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-1a9c84b55e85>\u001b[0m in \u001b[0;36mevaluate_prediction\u001b[0;34m(pred, labels, pad)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m### START CODE HERE (Replace instances of 'None' with your code) ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m## step 1 ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"outputs shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \"\"\"\n\u001b[0;32m-> 1188\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_prediction(NER(x), y, vocab['<PAD>'])\n",
    "print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcTJupqo5RcP"
   },
   "source": [
    "**Expected output (Approximately)**   \n",
    "```\n",
    "outputs shape: (7194, 70)\n",
    "mask shape: (7194, 70) mask[0][20:30]: [ True  True  True False False False False False False False]\n",
    "accuracy:  0.9543761281155191\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2FEleAFLr3r"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "# Part 5:  Testing with your own sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOeTPAx_Lr3t"
   },
   "source": [
    "Below, you can test it out with your own sentence! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0K4SyB20cHRf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is the function you will be using to test your own sentence.\n",
    "def predict(sentence, model, vocab, tag_map):\n",
    "    s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "    batch_data = np.ones((1, len(s)))\n",
    "    batch_data[0][:] = s\n",
    "    sentence = np.array(batch_data).astype(int)\n",
    "    output = model(sentence)\n",
    "    outputs = np.argmax(output, axis=2)\n",
    "    labels = list(tag_map.keys())\n",
    "    pred = []\n",
    "    for i in range(len(outputs[0])):\n",
    "        idx = outputs[0][i] \n",
    "        pred_label = labels[idx]\n",
    "        pred.append(pred_label)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "vLZCHoiULr3u",
    "outputId": "fab815fd-0472-4eaf-968a-abff1f5cfff5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try the output for the introduction example\n",
    "#sentence = \"Many French citizens are goin to visit Morocco for summer\"\n",
    "#sentence = \"Sharon Floyd flew to Miami last Friday\"\n",
    "\n",
    "# New york times news:\n",
    "sentence = \"Peter Navarro, the White House director of trade and manufacturing policy of U.S, said in an interview on Sunday morning that the White House was working to prepare for the possibility of a second wave of the coronavirus in the fall, though he said it wouldnt necessarily come\"\n",
    "s = [vocab[token] if token in vocab else vocab['UNK'] for token in sentence.split(' ')]\n",
    "predictions = predict(sentence, training_loop.model, vocab, tag_map)\n",
    "for x,y in zip(sentence.split(' '), predictions):\n",
    "    if y != 'O':\n",
    "        print(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHYbSnYKnCu6"
   },
   "source": [
    "** Expected Results **\n",
    "\n",
    "```\n",
    "Peter B-per\n",
    "Navarro, I-per\n",
    "White B-org\n",
    "House I-org\n",
    "Sunday B-tim\n",
    "morning I-tim\n",
    "White B-org\n",
    "House I-org\n",
    "coronavirus B-tim\n",
    "fall, B-tim\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C3_W3_Assignment_Solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "schema_names": [
    "NLPC3-3A"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
